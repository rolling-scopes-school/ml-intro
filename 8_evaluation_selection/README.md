> "Here is a system. I'm very sure that it's terrible." - *Yaser Abu-Mostafa*

Welcome to the evaluation and selection module of the RS School Machine Learning course. Hopefully, this module will help you to learn:
- How to select the best model and evaluate its performance.
- How to track your model experiments.
- How to organize your ML project.
- How to write a clean, reproducible, and flexible code.

This module has an assignment that is quite voluminous. Start working early to complete it before the deadline.

## Model evaluation and selection
1. A series of blog posts on model evaluation and selection from Sebastian Raschka (parts [1](https://sebastianraschka.com/blog/2016/model-evaluation-selection-part1.html), [2](https://sebastianraschka.com/blog/2016/model-evaluation-selection-part2.html), [3](https://sebastianraschka.com/blog/2016/model-evaluation-selection-part3.html), and [4](https://sebastianraschka.com/blog/2018/model-evaluation-selection-part4.html)). It discusses what evaluation and selection are and how to estimate their uncertainty. Feel free to skip any complicated maths, rather try to focus on what may be useful for you: holdout, k-fold, and nested cross-validation.
2. [Cross Validation in Time Series](https://medium.com/@soumyachess1496/cross-validation-in-time-series-566ae4981ce4). This article will tell you about validating time series models. However, the concept described there is very important and often used with any temporal features data, even if it's not time series. So study it carefully and try to understand well why we can't use a simple random split in this case.
3. Scikit learn [guide](https://scikit-learn.org/stable/model_selection.html) to cross-validation with code examples
4. Another [guide](https://weina.me/nested-cross-validation/) to nested cross-validation with a nice visualization. 
