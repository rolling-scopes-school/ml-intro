{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fortunately, with libraries such as Scikit-Learn, it’s now easy to build and use almost any machine learning algorithm. But it’s helpful to have an idea of how a machine learning model works under the hood. This lets us diagnose the model when it’s underperforming or explain how it makes decisions, which is crucial if we want to convince others to trust our models.\n",
    "In this assignment, we’ll look at how to build and use the Decision Tree and the Random Forest in Python. We’ll start by understanding how a single decision tree makes classifications on a simple problem. Then, we’ll work our way to using a random forest on a real-world data science problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset we will use in this assignment is the Sonar dataset.\n",
    "\n",
    "This is a dataset that describes sonar chirp returns bouncing off different surfaces. The 60 predictors are the strength of the returns at different angles. It is a binary classification problem that requires a model to differentiate rocks from metal cylinders. There are 208 observations.\n",
    "\n",
    "It is a well-understood dataset. All of the variables are continuous and generally in the range of 0 to 1. The output variable is a string “M” for mine and “R” for rock, which will need to be converted to integers 1 and 0.\n",
    "\n",
    "By predicting the class with the most observations in the dataset (M or mines) the Zero Rule Algorithm can achieve an accuracy of 53%.\n",
    "\n",
    "You can learn more about this dataset at the UCI Machine Learning repository.\n",
    "https://archive.ics.uci.edu/ml/datasets/Connectionist+Bench+(Sonar,+Mines+vs.+Rocks)\n",
    "\n",
    "Download the dataset for free and place it in the \"data\" folder in your working directory with the filename sonar.all-data.csv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T08:48:35.022113Z",
     "start_time": "2020-12-01T08:48:34.335814Z"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "np.random.seed(2020)\n",
    "random.seed(2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data and convert targets to integers 1 and 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T08:48:35.618412Z",
     "start_time": "2020-12-01T08:48:35.579547Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feat_0</th>\n",
       "      <th>feat_1</th>\n",
       "      <th>feat_2</th>\n",
       "      <th>feat_3</th>\n",
       "      <th>feat_4</th>\n",
       "      <th>feat_5</th>\n",
       "      <th>feat_6</th>\n",
       "      <th>feat_7</th>\n",
       "      <th>feat_8</th>\n",
       "      <th>feat_9</th>\n",
       "      <th>...</th>\n",
       "      <th>feat_51</th>\n",
       "      <th>feat_52</th>\n",
       "      <th>feat_53</th>\n",
       "      <th>feat_54</th>\n",
       "      <th>feat_55</th>\n",
       "      <th>feat_56</th>\n",
       "      <th>feat_57</th>\n",
       "      <th>feat_58</th>\n",
       "      <th>feat_59</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0371</td>\n",
       "      <td>0.0428</td>\n",
       "      <td>0.0207</td>\n",
       "      <td>0.0954</td>\n",
       "      <td>0.0986</td>\n",
       "      <td>0.1539</td>\n",
       "      <td>0.1601</td>\n",
       "      <td>0.3109</td>\n",
       "      <td>0.2111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0065</td>\n",
       "      <td>0.0159</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0167</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0090</td>\n",
       "      <td>0.0032</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0453</td>\n",
       "      <td>0.0523</td>\n",
       "      <td>0.0843</td>\n",
       "      <td>0.0689</td>\n",
       "      <td>0.1183</td>\n",
       "      <td>0.2583</td>\n",
       "      <td>0.2156</td>\n",
       "      <td>0.3481</td>\n",
       "      <td>0.3337</td>\n",
       "      <td>0.2872</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0084</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0.0191</td>\n",
       "      <td>0.0140</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0052</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0262</td>\n",
       "      <td>0.0582</td>\n",
       "      <td>0.1099</td>\n",
       "      <td>0.1083</td>\n",
       "      <td>0.0974</td>\n",
       "      <td>0.2280</td>\n",
       "      <td>0.2431</td>\n",
       "      <td>0.3771</td>\n",
       "      <td>0.5598</td>\n",
       "      <td>0.6194</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0232</td>\n",
       "      <td>0.0166</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0180</td>\n",
       "      <td>0.0244</td>\n",
       "      <td>0.0316</td>\n",
       "      <td>0.0164</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0078</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0171</td>\n",
       "      <td>0.0623</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0205</td>\n",
       "      <td>0.0368</td>\n",
       "      <td>0.1098</td>\n",
       "      <td>0.1276</td>\n",
       "      <td>0.0598</td>\n",
       "      <td>0.1264</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0150</td>\n",
       "      <td>0.0085</td>\n",
       "      <td>0.0073</td>\n",
       "      <td>0.0050</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.0040</td>\n",
       "      <td>0.0117</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0762</td>\n",
       "      <td>0.0666</td>\n",
       "      <td>0.0481</td>\n",
       "      <td>0.0394</td>\n",
       "      <td>0.0590</td>\n",
       "      <td>0.0649</td>\n",
       "      <td>0.1209</td>\n",
       "      <td>0.2467</td>\n",
       "      <td>0.3564</td>\n",
       "      <td>0.4459</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0105</td>\n",
       "      <td>0.0110</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0072</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0107</td>\n",
       "      <td>0.0094</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   feat_0  feat_1  feat_2  feat_3  feat_4  feat_5  feat_6  feat_7  feat_8  \\\n",
       "0  0.0200  0.0371  0.0428  0.0207  0.0954  0.0986  0.1539  0.1601  0.3109   \n",
       "1  0.0453  0.0523  0.0843  0.0689  0.1183  0.2583  0.2156  0.3481  0.3337   \n",
       "2  0.0262  0.0582  0.1099  0.1083  0.0974  0.2280  0.2431  0.3771  0.5598   \n",
       "3  0.0100  0.0171  0.0623  0.0205  0.0205  0.0368  0.1098  0.1276  0.0598   \n",
       "4  0.0762  0.0666  0.0481  0.0394  0.0590  0.0649  0.1209  0.2467  0.3564   \n",
       "\n",
       "   feat_9  ...  feat_51  feat_52  feat_53  feat_54  feat_55  feat_56  feat_57  \\\n",
       "0  0.2111  ...   0.0027   0.0065   0.0159   0.0072   0.0167   0.0180   0.0084   \n",
       "1  0.2872  ...   0.0084   0.0089   0.0048   0.0094   0.0191   0.0140   0.0049   \n",
       "2  0.6194  ...   0.0232   0.0166   0.0095   0.0180   0.0244   0.0316   0.0164   \n",
       "3  0.1264  ...   0.0121   0.0036   0.0150   0.0085   0.0073   0.0050   0.0044   \n",
       "4  0.4459  ...   0.0031   0.0054   0.0105   0.0110   0.0015   0.0072   0.0048   \n",
       "\n",
       "   feat_58  feat_59  target  \n",
       "0   0.0090   0.0032       0  \n",
       "1   0.0052   0.0044       0  \n",
       "2   0.0095   0.0078       0  \n",
       "3   0.0040   0.0117       0  \n",
       "4   0.0107   0.0094       0  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = 'data/'\n",
    "df = pd.read_csv(PATH+'sonar-all-data.csv', header=None)\n",
    "df.columns = [f'feat_{col}' if col!=60 else 'target' for col in df.columns]\n",
    "df['target'] = df['target'].map({'M': 1, 'R': 0})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data (train and test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T08:48:36.554013Z",
     "start_time": "2020-12-01T08:48:36.548028Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.drop(columns='target'), df['target'], test_size=0.2, random_state=2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cost functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section you should implement two cost functions. Any of these can be used in the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gini index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T08:48:38.113015Z",
     "start_time": "2020-12-01T08:48:38.108028Z"
    }
   },
   "outputs": [],
   "source": [
    "def gini_index(x):\n",
    "    \"\"\" Calculate Gini Index for a node\n",
    "    Args:\n",
    "        x: Numpy-array of targets in a node\n",
    "    Returns:\n",
    "        float: Gini index\n",
    "    \"\"\"    \n",
    "    if len(x) == 0:\n",
    "        return 0.0\n",
    "    p = np.bincount(x) / len(x)\n",
    "    return 1 - np.sum(p*p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T08:48:38.326622Z",
     "start_time": "2020-12-01T08:48:38.320603Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4977348372781065"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = df['target'].values\n",
    "gini_index(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T08:48:38.517903Z",
     "start_time": "2020-12-01T08:48:38.512916Z"
    }
   },
   "outputs": [],
   "source": [
    "def gini_gain(parent_node, splits):\n",
    "    \"\"\" Calculate Gini Gain for a particular split\n",
    "    Args:\n",
    "        parent_node: Numpy-array of targets in a parent node\n",
    "        splits: List of two numpy-arrays. Each numpy-array is targets in a child node\n",
    "    Returns:\n",
    "        float: Gini gain\n",
    "    \"\"\"       \n",
    "    splits_gini = np.sum([gini_index(split)*(len(split)/len(parent_node)) for split in splits])\n",
    "    return gini_index(parent_node) - splits_gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T08:48:38.731542Z",
     "start_time": "2020-12-01T08:48:38.719612Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0003705758273065962"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits = [np.random.choice(df['target'].values, 100), np.random.choice(df['target'].values, 108)]\n",
    "gini_gain(target, splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T08:48:39.080145Z",
     "start_time": "2020-12-01T08:48:39.075122Z"
    }
   },
   "outputs": [],
   "source": [
    "def entropy(x):\n",
    "    \"\"\" Calculate Entropy for a node\n",
    "    Args:\n",
    "        x: Numpy-array of targets in a node\n",
    "    Returns:\n",
    "        float: Entropy\n",
    "    \"\"\"\n",
    "    if len(x) == 0:\n",
    "        return 0.0\n",
    "    p = np.clip(np.bincount(x) / len(x), 1e-15, 1.)\n",
    "    return -np.sum(p * np.log(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T08:48:39.248336Z",
     "start_time": "2020-12-01T08:48:39.243349Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6908803044104659"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entropy(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T08:48:39.415230Z",
     "start_time": "2020-12-01T08:48:39.410243Z"
    }
   },
   "outputs": [],
   "source": [
    "def information_gain(parent_node, splits):\n",
    "    \"\"\" Calculate Information Gain for a particular split\n",
    "    Args:\n",
    "        parent_node: Numpy-array of targets in a parent node\n",
    "        splits: List of two numpy-arrays. Each numpy-array is targets in a child node\n",
    "    Returns:\n",
    "        float: Information Gain\n",
    "    \"\"\"     \n",
    "    splits_entropy = np.sum([entropy(split)*(len(split)/len(parent_node)) for split in splits])\n",
    "    return entropy(parent_node) - splits_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T08:48:39.576789Z",
     "start_time": "2020-12-01T08:48:39.569809Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.0003705625503662713"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "information_gain(target, splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement split functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T08:48:40.068764Z",
     "start_time": "2020-12-01T08:48:40.063788Z"
    }
   },
   "outputs": [],
   "source": [
    "def split(X, y, value):\n",
    "    \"\"\" Split y-values in order to calculate gain later\n",
    "    Args:\n",
    "        X: 1-dimensional numpy-array of data predictor with shape (N,)\n",
    "        y: 1-dimensional numpy-array of targets with shape (N,)\n",
    "        value (float): the value by which the X should be splitted\n",
    "    Returns:\n",
    "        Two 1-dimensional numpy-arrays with targets related to splits\n",
    "    \"\"\"      \n",
    "    left_mask = X < value\n",
    "    right_mask = X >= value\n",
    "    return y[left_mask], y[right_mask]\n",
    "\n",
    "\n",
    "def split_dataset(X, y, column, value):\n",
    "    \"\"\" Split dataset by a particular column and value\n",
    "    Args:\n",
    "        X: 2-dimensional numpy-array (N, num_feats). N-number of samples\n",
    "        y: 1-dimensional numpy-array of targets with shape (N,)  \n",
    "        column (int): the column by which the X should be splitted\n",
    "        value (float): the value by which the column should be splitted\n",
    "    Returns:\n",
    "        Two 2-dimensional numpy-arrays with data and two 1-dimensional numpy-arrays with targets related to splits\n",
    "        left_X, right_X, left_y, right_y\n",
    "    \"\"\"       \n",
    "    left_mask = X[:, column] < value\n",
    "    right_mask = X[:, column] >= value\n",
    "    left_y, right_y = y[left_mask], y[right_mask]\n",
    "    left_X, right_X = X[left_mask], X[right_mask]\n",
    "    return left_X, right_X, left_y, right_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T08:48:40.412627Z",
     "start_time": "2020-12-01T08:48:40.393648Z"
    }
   },
   "outputs": [],
   "source": [
    "class Tree(object):\n",
    "    \"\"\"A decision tree classifier.\n",
    "\n",
    "    Args:\n",
    "        criterion : {\"gini_gain\", \"information_gain\"}\n",
    "    \"\"\"\n",
    "    def __init__(self, criterion=None):\n",
    "        self.impurity = None\n",
    "        self.threshold = None\n",
    "        self.column_index = None\n",
    "        self.outcome_probs = None\n",
    "        self.criterion = criterion\n",
    "        self.left_child = None\n",
    "        self.right_child = None\n",
    "\n",
    "    @property\n",
    "    def is_terminal(self):\n",
    "        \"\"\" Define is it terminal node\n",
    "        \"\"\"          \n",
    "        return not bool(self.left_child and self.right_child)\n",
    "\n",
    "    def _find_splits(self, X):\n",
    "        \"\"\"Find all possible split values.\"\"\"\n",
    "        split_values = set()\n",
    "\n",
    "        # Get unique values in a sorted order\n",
    "        x_unique = list(np.unique(X))\n",
    "        for i in range(1, len(x_unique)):\n",
    "            # Find a point between two values\n",
    "            average = (x_unique[i - 1] + x_unique[i]) / 2.0\n",
    "            split_values.add(average)\n",
    "\n",
    "        return list(split_values)\n",
    "\n",
    "    def _find_best_split(self, X, y, n_features):\n",
    "        \"\"\"Find best feature and value for a split. Greedy algorithm.\"\"\"\n",
    "\n",
    "        # Sample random subset of features\n",
    "        subset = random.sample(list(range(0, X.shape[1])), n_features)\n",
    "        max_gain, max_col, max_val = None, None, None\n",
    "\n",
    "        for column in subset:\n",
    "            split_values = self._find_splits(X[:, column])\n",
    "            for value in split_values:\n",
    "                splits = split(X[:, column], y, value)\n",
    "                gain = self.criterion(y, splits)\n",
    "\n",
    "                if (max_gain is None) or (gain > max_gain):\n",
    "                    max_col, max_val, max_gain = column, value, gain\n",
    "        return max_col, max_val, max_gain\n",
    "\n",
    "    def fit(self, X, y, n_features=None, max_depth=None):\n",
    "        \"\"\"Fit model.\n",
    "\n",
    "        Args:\n",
    "            X (numpy-array): The training input samples. 2-dimensional numpy array.\n",
    "            y (numpy-array): The target values. 1-dimensional numpy array.\n",
    "            n_features (int): The number of features when fit is performed (default: all features)\n",
    "            max_depth (int): The maximum depth of the tree. If None, then nodes are expanded until\n",
    "                             all leaves are pure.\n",
    "        \"\"\"        \n",
    "        try:\n",
    "            # Exit from recursion using assert syntax\n",
    "            if max_depth is not None:\n",
    "                assert max_depth > 0\n",
    "                max_depth -= 1\n",
    "\n",
    "            if n_features is None:\n",
    "                n_features = X.shape[1]\n",
    "\n",
    "            column, value, gain = self._find_best_split(X, y, n_features)\n",
    "            assert gain is not None\n",
    "\n",
    "            self.column_index = column\n",
    "            self.threshold = value\n",
    "            self.impurity = gain\n",
    "\n",
    "            # Split dataset\n",
    "            left_X, right_X, left_target, right_target = split_dataset(X, y, column, value)\n",
    "\n",
    "            # Grow left and right child\n",
    "            self.left_child = Tree(self.criterion)\n",
    "            self.left_child.fit(\n",
    "                left_X, left_target, n_features, max_depth\n",
    "            )\n",
    "\n",
    "            self.right_child = Tree(self.criterion)\n",
    "            self.right_child.fit(\n",
    "                right_X, right_target, n_features, max_depth\n",
    "            )\n",
    "        except AssertionError:\n",
    "            self.outcome_probs = np.around(np.sum(y) / y.shape[0])\n",
    "\n",
    "\n",
    "    def predict_row(self, row):\n",
    "        \"\"\"Predict single row.\"\"\"\n",
    "        if not self.is_terminal:\n",
    "            if row[self.column_index] < self.threshold:\n",
    "                return self.left_child.predict_row(row)\n",
    "            else:\n",
    "                return self.right_child.predict_row(row)\n",
    "        return self.outcome_probs\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions.\n",
    "\n",
    "        Args:\n",
    "            X (numpy-array): The test input samples. 2-dimensional numpy array.\n",
    "        \"\"\"  \n",
    "        result = np.zeros(X.shape[0])\n",
    "        for i in range(X.shape[0]):\n",
    "            result[i] = self.predict_row(X[i, :])\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit two models with \"max_depth=3\" and \"max_depth=None\" hyperparameters. Explain the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T08:48:45.836138Z",
     "start_time": "2020-12-01T08:48:41.916791Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score is: 0.6428571428571429\n"
     ]
    }
   ],
   "source": [
    "model = Tree(criterion=gini_gain)\n",
    "model.fit(X_train.values, y_train.values)\n",
    "y_pred = model.predict(X_test.values)\n",
    "print(f\"Accuracy score is: {accuracy_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T08:48:41.915820Z",
     "start_time": "2020-12-01T08:48:40.891474Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score is: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "model = Tree(criterion=gini_gain)\n",
    "model.fit(X_train.values, y_train.values, max_depth=3)\n",
    "y_pred = model.predict(X_test.values)\n",
    "print(f\"Accuracy score is: {accuracy_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T08:48:45.857086Z",
     "start_time": "2020-12-01T08:48:45.838139Z"
    }
   },
   "outputs": [],
   "source": [
    "class RandomForestClassifier(object):\n",
    "    \"\"\"\n",
    "    A random forest classifier.\n",
    "    A random forest is a meta estimator that fits a number of decision tree\n",
    "    classifiers on various sub-samples of the dataset and uses averaging to\n",
    "    improve the predictive accuracy and control overfitting.\n",
    "    \n",
    "    Args:\n",
    "        n_estimators : int, default=10\n",
    "            The number of trees in the forest.\n",
    "\n",
    "        max_depth : int, default=None\n",
    "            The maximum depth of the tree. If None, then nodes are expanded until\n",
    "            all leaves are pure.        \n",
    "\n",
    "        n_features : int, default=None\n",
    "            The number of features to consider when looking for the best split.\n",
    "            If None, then `n_features=sqrt(n_features)`.\n",
    "\n",
    "        criterion : {\"gini\", \"entropy\"}, default=\"gini\"\n",
    "            The function to measure the quality of a split. Supported criteria are\n",
    "            \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_estimators=10, max_depth=None, n_features=None, criterion=\"entropy\", bootstrap=True):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.n_features = n_features\n",
    "        self.bootstrap = bootstrap\n",
    "        \n",
    "        if criterion == \"entropy\":\n",
    "            self.criterion = information_gain\n",
    "        elif criterion == \"gini\":\n",
    "            self.criterion = gini_gain\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown criterion '{criterion}'\")\n",
    "            \n",
    "        self.trees = [Tree(criterion=self.criterion) for _ in range(n_estimators)]\n",
    "        \n",
    "    def _init_data(self, X, y):\n",
    "        \"\"\"Ensure data are in the expected format.\n",
    "        Ensures X and y are stored as numpy ndarrays by converting from an\n",
    "        array-like object if necessary. \n",
    "        Parameters\n",
    "        Args:\n",
    "            X : array-like\n",
    "                Feature dataset.\n",
    "            y : array-like, default=None\n",
    "                Target values. By default is required, but if y_required = false\n",
    "                then may be omitted.\n",
    "        \"\"\"\n",
    "        self.size = len(X)\n",
    "        \n",
    "        if not isinstance(X, np.ndarray):\n",
    "            self.X = np.array(X)\n",
    "        else:\n",
    "            self.X = X\n",
    "\n",
    "        if not isinstance(y, np.ndarray):\n",
    "            self.y = np.array(y)\n",
    "        else:\n",
    "            self.y = y\n",
    "            \n",
    "    def bootstrap_data(self, size):\n",
    "        return np.random.randint(size, size=size)\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Fit model.\n",
    "\n",
    "        Args:\n",
    "            X (numpy-array): The training input samples. 2-dimensional numpy array.\n",
    "            y (numpy-array): The target values. 1-dimensional numpy array.\n",
    "        \"\"\"         \n",
    "        if self.n_features is None:\n",
    "            self.n_features = int(np.sqrt(X.shape[1]))\n",
    "        elif X.shape[1] < self.n_features:\n",
    "            raise ValueError(f\"'n_features should be <= n_features'\")\n",
    "            \n",
    "        self._init_data(X, y)\n",
    "        \n",
    "        for tree in self.trees:\n",
    "            if self.bootstrap:\n",
    "                idxs = self.bootstrap_data(self.size)\n",
    "                X = self.X[idxs]\n",
    "                y = self.y[idxs]\n",
    "            else:\n",
    "                X = self.X\n",
    "                y = self.y\n",
    "                \n",
    "            tree.fit(\n",
    "                X,\n",
    "                y,\n",
    "                n_features=self.n_features,\n",
    "                max_depth=self.max_depth,\n",
    "            )\n",
    "            \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions.\n",
    "\n",
    "        Args:\n",
    "            X (numpy-array): The test data input samples. 2-dimensional numpy array.\n",
    "        \"\"\"            \n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = np.array(X)\n",
    "\n",
    "        if self.X is not None:\n",
    "            predictions = np.zeros(len(X))\n",
    "            for i in range(len(X)):\n",
    "                row_pred = 0.\n",
    "                for tree in self.trees:\n",
    "                    row_pred += tree.predict_row(X[i, :])\n",
    "\n",
    "                row_pred /= self.n_estimators\n",
    "                predictions[i] = round(row_pred)\n",
    "            return predictions  \n",
    "        else:\n",
    "            raise ValueError(\"You should fit a model before `predict`\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit two models with \"n_estimators=10\" and \"n_estimators=100\" hyperparameters. Explain the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T08:48:51.641967Z",
     "start_time": "2020-12-01T08:48:45.860076Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score is: 0.7619047619047619\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(n_estimators=10, max_depth=None, n_features=None, criterion=\"entropy\")\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(f\"Accuracy score is: {accuracy_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-01T08:49:47.690531Z",
     "start_time": "2020-12-01T08:48:51.644919Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score is: 0.8333333333333334\n"
     ]
    }
   ],
   "source": [
    "model = RandomForestClassifier(n_estimators=100, max_depth=None, n_features=None, criterion=\"entropy\")\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(f\"Accuracy score is: {accuracy_score(y_test, y_pred)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:wpm_matching_py373]",
   "language": "python",
   "name": "conda-env-wpm_matching_py373-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "235px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
