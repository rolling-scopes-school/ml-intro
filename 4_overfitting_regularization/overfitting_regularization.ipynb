{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfitting andÂ Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-16T07:39:25.890581Z",
     "start_time": "2022-02-16T07:39:23.835921Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-16T07:39:26.108031Z",
     "start_time": "2022-02-16T07:39:25.925893Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import set_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-16T07:39:26.155183Z",
     "start_time": "2022-02-16T07:39:26.141249Z"
    }
   },
   "outputs": [],
   "source": [
    "set_config(display='diagram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-16T07:39:26.201661Z",
     "start_time": "2022-02-16T07:39:26.189181Z"
    }
   },
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "RANGE = (-5, 5)\n",
    "N_SAMPLES = 50\n",
    "DEGREES = np.linspace(0, 15, 1 + 15, dtype=int)\n",
    "ALPHAS = np.linspace(0, 0.5, 1 + 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Underfitting vs. overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T09:04:09.904994Z",
     "start_time": "2020-11-09T09:04:09.896444Z"
    }
   },
   "source": [
    "Let's pick a target function $ f(x) = 2\\cdot x + 10\\cdot sin(x) $ and generate some noisy samples to learn from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-16T07:39:26.433239Z",
     "start_time": "2022-02-16T07:39:26.412501Z"
    }
   },
   "outputs": [],
   "source": [
    "def target_function(x):\n",
    "    return 2 * x + 10 * np.sin(x)\n",
    "\n",
    "def generate_samples():\n",
    "    \"\"\"Generate noisy samples.\"\"\"\n",
    "    np.random.seed(SEED)\n",
    "    x = np.random.uniform(*RANGE, size=N_SAMPLES)\n",
    "    y = target_function(x) + np.random.normal(scale=4, size=N_SAMPLES)\n",
    "    return x.reshape(-1, 1), y\n",
    "\n",
    "X, y = generate_samples()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-16T07:39:27.822431Z",
     "start_time": "2022-02-16T07:39:27.696426Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_scatter(x, y, title=None, label='Noisy samples'):\n",
    "    plt.scatter(x, y, label=label)\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.grid(True)\n",
    "    plt.title(title)\n",
    "    plt.legend(loc='lower right')\n",
    "\n",
    "plot_scatter(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-16T07:39:29.210072Z",
     "start_time": "2022-02-16T07:39:29.093175Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.3, random_state=SEED)\n",
    "\n",
    "plot_scatter(X_train, y_train, label='Training set')\n",
    "plot_scatter(X_valid, y_valid, label='Validation set')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-16T07:39:33.496625Z",
     "start_time": "2022-02-16T07:39:33.479851Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to approximate our target function $ f(x) = 2\\cdot x + 10\\cdot sin(x) $ with polynomials of different degree. \n",
    "\n",
    "A polynomial of degree $n$ has the form:\n",
    "$ h(x) = w_0 + w_1\\cdot x + w_2\\cdot x^2 +\\ldots + w_n\\cdot x^n $.\n",
    "\n",
    "$x^i$ values could easily be generated by `PolynomialFeatures`, while $w_i$ are the unknown paramaters to be estimated using `LinearRegression`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-16T07:39:34.647226Z",
     "start_time": "2022-02-16T07:39:34.639540Z"
    }
   },
   "outputs": [],
   "source": [
    "PolynomialFeatures(degree=4, include_bias=False).fit_transform(X=[\n",
    "    [1],\n",
    "    [3],\n",
    "    [4],\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-16T07:40:49.993908Z",
     "start_time": "2022-02-16T07:40:49.959340Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_model(degree, alpha=0, penalty=None):\n",
    "    # linear regression\n",
    "    if alpha == 0:\n",
    "        regressor = LinearRegression()\n",
    "    # lasso regression\",\n",
    "    elif penalty == 'L1':\n",
    "        regressor = Lasso(alpha=alpha, random_state=SEED, max_iter=50000)\n",
    "    # ridge regression\",\n",
    "    elif penalty == 'L2':\n",
    "        regressor = Ridge(alpha=alpha, random_state=SEED, max_iter=50000) \n",
    "    \n",
    "    \n",
    "    return Pipeline([\n",
    "        ('pol', PolynomialFeatures(degree, include_bias=(degree == 0))),\n",
    "        ('sca', StandardScaler()),\n",
    "        ('reg', regressor)\n",
    "    ])\n",
    "\n",
    "display(make_model(2))\n",
    "display(make_model(2, penalty='L1', alpha=0.1))\n",
    "display(make_model(2, penalty='L2', alpha=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fit a model and plot the hypothesis it learns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-16T07:40:51.426785Z",
     "start_time": "2022-02-16T07:40:51.209830Z"
    }
   },
   "outputs": [],
   "source": [
    "def plot_fit(model):\n",
    "    degree = model['pol'].degree\n",
    "    X_range = np.linspace(*RANGE, 1000).reshape(-1, 1)\n",
    "    y_pred = model.predict(X_range)\n",
    "    plot_scatter(X_train, y_train, label='Training sample')\n",
    "    plot_scatter(X_valid, y_valid, label='Validation sample')\n",
    "    plt.plot(X_range, target_function(X_range), c='green', alpha=0.2, lw=5, label='Target function')\n",
    "    plt.plot(X_range, y_pred, c='red', label='Hypothesis')\n",
    "    plt.ylim((min(y) - 3, max(y) + 3))\n",
    "    plt.legend(loc='best')    \n",
    "    plt.title(f'Polynomial approximation: degree={degree}')\n",
    "    plt.show()\n",
    "\n",
    "plot_fit(make_model(degree=2).fit(X_train, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From underfitting to overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T11:15:24.323458Z",
     "start_time": "2020-11-09T11:15:24.318089Z"
    }
   },
   "source": [
    "We can investigate the shape of the fitted curve for different values of `degree`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-16T07:40:54.236522Z",
     "start_time": "2022-02-16T07:40:52.748262Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for degree in [0, 1, 2, 3, 4, 5, 10, 15, 20]:\n",
    "    plot_fit(make_model(degree).fit(X_train, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step we calculate the training and the validation error for each `degree` and plot them in a single graph. The resulting graph is called the fitting graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-15T20:32:47.308471Z",
     "start_time": "2021-12-15T20:32:47.036535Z"
    }
   },
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "def plot_fitting_graph(x, metric_train, metric_valid, xlabel, ylabel, \n",
    "                       custom_metric=None, custom_label='', custom_scale='log', title='Fitting graph'):\n",
    "    plt.figure(figsize=(9, 4.5))\n",
    "    plt.plot(x, metric_train, label='Training')\n",
    "    plt.plot(x, metric_valid, color='C1', label='Validation')\n",
    "    plt.axvline(x[np.argmin(metric_valid)], color='C1', lw=10, alpha=0.2)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.grid(True)\n",
    "    plt.xticks(x, rotation='vertical')\n",
    "    plt.legend(loc='center left')        \n",
    "    if custom_metric:\n",
    "        plt.twinx()\n",
    "        plt.yscale(custom_scale)\n",
    "        plt.plot(x, custom_metric, alpha=0.2, lw=4, ls='dotted', color='black', label=custom_label) \n",
    "        plt.legend(loc='center right')         \n",
    "    plt.show()\n",
    "    \n",
    "rmse_train, rmse_valid = [], []\n",
    "for degree in DEGREES:\n",
    "    reg = make_model(degree).fit(X_train, y_train)\n",
    "    rmse_train.append(rmse(reg.predict(X_train), y_train))\n",
    "    rmse_valid.append(rmse(reg.predict(X_valid), y_valid))\n",
    "    \n",
    "plot_fitting_graph(DEGREES, rmse_train, rmse_valid, xlabel='Complexity (degree)', ylabel='Error (RMSE)', \n",
    "                   title='Least squares polynomial regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sweet spot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the optimal `degree` to go with?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-15T20:32:47.324492Z",
     "start_time": "2021-12-15T20:32:47.308471Z"
    }
   },
   "outputs": [],
   "source": [
    "DEGREES[np.argmin(rmse_valid)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ideally, we would choose the the model parameters such that we have the best model performance. However, we want to make sure that we really have the best validation performance. When we do `train_test_split` we randomly split the data into two parts. What could happen is that we got lucky and split the data such that it favours the validation error. This is especially dangerous if we are dealing with small datasets. One way to check if that's the case is to run the experiment several times for different, random splits. However, there is an even more systematic way of doing this: [cross-validation](https://scikit-learn.org/stable/modules/cross_validation.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://scikit-learn.org/stable/_images/grid_search_cross_validation.png\" width=50% />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-15T20:32:47.788153Z",
     "start_time": "2021-12-15T20:32:47.324492Z"
    }
   },
   "outputs": [],
   "source": [
    "rmse_train, rmse_valid = [], []\n",
    "for degree in DEGREES:\n",
    "    results = cross_validate(make_model(degree), \n",
    "                             X, y, cv=5,\n",
    "                             return_train_score=True,\n",
    "                             scoring='neg_root_mean_squared_error')\n",
    "    rmse_train.append(-np.mean(results['train_score']))\n",
    "    rmse_valid.append(-np.mean(results['test_score']))\n",
    "    \n",
    "plot_fitting_graph(DEGREES, rmse_train, rmse_valid, xlabel='Complexity (degree)', ylabel='Error (RMSE)',\n",
    "                   title='Least squares polynomial regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-05T16:26:33.200639Z",
     "start_time": "2020-11-05T16:26:33.197656Z"
    }
   },
   "source": [
    "Let's inspect our regression model coefficients:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-15T20:32:47.820142Z",
     "start_time": "2021-12-15T20:32:47.788153Z"
    }
   },
   "outputs": [],
   "source": [
    "(make_model(degree=1).fit(X_train, y_train)['reg'].coef_,\n",
    " make_model(degree=2).fit(X_train, y_train)['reg'].coef_,\n",
    " make_model(degree=5).fit(X_train, y_train)['reg'].coef_,\n",
    " make_model(degree=10).fit(X_train, y_train)['reg'].coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm... it looks like high degree polynomials are coming with much bigger regression coefficients. \n",
    "\n",
    "We are going to plot the mean absolute value of $w_i$ as a function of degree to reveal the relationship:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-15T20:32:48.876099Z",
     "start_time": "2021-12-15T20:32:47.820142Z"
    }
   },
   "outputs": [],
   "source": [
    "rmse_train, rmse_valid, avg_coef = [], [], []\n",
    "for degree in DEGREES:\n",
    "    results = cross_validate(make_model(degree),\n",
    "                             X, y, cv=5,\n",
    "                             return_train_score=True, return_estimator=True,\n",
    "                             scoring='neg_root_mean_squared_error')\n",
    "    rmse_train.append(-np.mean(results['train_score']))\n",
    "    rmse_valid.append(-np.mean(results['test_score']))        \n",
    "    avg_coef.append(        \n",
    "        # average over CV folds\n",
    "        np.mean([            \n",
    "            # mean absolute value of weights\n",
    "            np.mean(np.abs(model['reg'].coef_))\n",
    "            for model in results['estimator']\n",
    "        ]))\n",
    "    \n",
    "plot_fitting_graph(DEGREES, rmse_train, rmse_valid,\n",
    "                   xlabel='Complexity (degree)', ylabel='Error (RMSE)',\n",
    "                   custom_metric=avg_coef, custom_label='avg(|$w_i$|)',\n",
    "                   title='Least squares polynomial regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T15:24:18.245064Z",
     "start_time": "2020-11-09T15:24:18.241548Z"
    }
   },
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe the following:\n",
    "\n",
    "1. **Underfitting** (degree < 5): The model is not able to fit the data properly. The fit is bad for both the training and the validation set.\n",
    "\n",
    "2. **Fit is just right** (degree = 5): The model is able to capture the underlying data distribution. The fit is good for both the training and the validation set.\n",
    "\n",
    "3. **Overfitting** (degree > 5): The model starts fitting the noise in the dataset. While the fit for the training data gets even better, the fit for the validation set gets worse.\n",
    "\n",
    "4. As the order of polynomial increases, the linear model coefficients become more likely to take on **large values**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T08:47:28.699189Z",
     "start_time": "2020-11-03T08:47:28.695466Z"
    }
   },
   "source": [
    "## Part 2: Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two major ways to build a machine learning model with the ability to generalize well on unseen data:\n",
    "1. Train the simplest model possible for our purpose (according to Occamâs Razor).\n",
    "2. Train a complex or more expressive model on the data and perform regularization.\n",
    "\n",
    "Regularization is a method used to reduce the variance of a machine learning model. In other words, it is used to reduce overfitting. Regularization penalizes a model for being complex. For linear models, it means regularization forces model coefficients to be smaller in magnitude.\n",
    "\n",
    "Let's pick a polynomial model of degree **15** (which tends to overfit strongly) and try to regularize it using **L1** and **L2** penalties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1 - Lasso regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-15T20:32:49.851006Z",
     "start_time": "2021-12-15T20:32:48.876099Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rmse_train, rmse_valid = [], []\n",
    "for alpha in ALPHAS:    \n",
    "    results = cross_validate(make_model(degree=15, penalty='L1', alpha=alpha), \n",
    "                             X, y, cv=5,\n",
    "                             return_train_score=True,\n",
    "                             scoring='neg_root_mean_squared_error')\n",
    "    rmse_train.append(-np.mean(results['train_score']))\n",
    "    rmse_valid.append(-np.mean(results['test_score']))\n",
    "    \n",
    "plot_fitting_graph(ALPHAS, rmse_train, rmse_valid,\n",
    "                   xlabel='Regularization strength (alpha)', ylabel='Error (RMSE)',\n",
    "                   title='Lasso polynomial regression (L1): degree=15')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L2 - Ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-15T20:32:51.202745Z",
     "start_time": "2021-12-15T20:32:50.468099Z"
    }
   },
   "outputs": [],
   "source": [
    "rmse_train, rmse_valid = [], []\n",
    "for alpha in ALPHAS:    \n",
    "    results = cross_validate(make_model(degree=15, penalty='L2', alpha=alpha), \n",
    "                             X, y, cv=5,\n",
    "                             return_train_score=True,\n",
    "                             scoring='neg_root_mean_squared_error')\n",
    "    rmse_train.append(-np.mean(results['train_score']))\n",
    "    rmse_valid.append(-np.mean(results['test_score']))\n",
    "    \n",
    "plot_fitting_graph(ALPHAS, rmse_train, rmse_valid, \n",
    "                   xlabel='Regularization strength (alpha)', ylabel='Error (RMSE)', \n",
    "                   title='Ridge polynomial regression (L2): degree=15')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T13:47:39.048589Z",
     "start_time": "2020-11-09T13:47:39.044912Z"
    }
   },
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T13:49:03.993455Z",
     "start_time": "2020-11-09T13:49:03.987472Z"
    }
   },
   "source": [
    "1. We can control the regularization strength by changing the hyperparameter `alpha`.\n",
    "2. Regularized version of the model performs pretty well. Even in case the original original (unregularized) model is heavily overfitting due to excessive complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-09T12:14:17.962945Z",
     "start_time": "2020-11-09T12:14:17.959952Z"
    }
   },
   "source": [
    "## Part 3: Homework assignment (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-10T12:27:23.202301Z",
     "start_time": "2021-12-10T12:27:23.185315Z"
    }
   },
   "source": [
    "### Excercise 1 - Overfiting and Underfitting (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-10T07:35:07.485715Z",
     "start_time": "2021-12-10T07:35:07.461799Z"
    }
   },
   "source": [
    "Let's work with the diabetes dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-15T20:32:51.450770Z",
     "start_time": "2021-12-15T20:32:51.234725Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "data = load_diabetes()\n",
    "X_diabetes = pd.DataFrame(data['data'], columns=data['feature_names'])\n",
    "y_diabetes = pd.DataFrame(data['target'], columns=['target'])\n",
    "print(data['DESCR'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply model for diabetes dataset with polynomial feature engineering of different degrees. Plot the dependence of train and test error on polynomial degree. Highlight a degree with the best test error. Which degrees cause overfitting/underfitting? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-15T20:38:03.456910Z",
     "start_time": "2021-12-15T20:38:03.448915Z"
    }
   },
   "outputs": [],
   "source": [
    "# your findings/conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-10T12:46:46.756169Z",
     "start_time": "2021-12-10T12:44:13.217Z"
    }
   },
   "source": [
    "### Excercise 2 - Magnitude (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As discussed earlier, regularization methods are expected to constraint the weights (model coefficients). \n",
    "\n",
    "Is it indeed happening? \n",
    "\n",
    "Please do a discovery on your own and find that out empirically (both for **L1** and **L2**). Let's use `degree=15` and `alpha` from `ALPHAS`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your observations/conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-10T12:46:46.756169Z",
     "start_time": "2021-12-10T12:44:13.217Z"
    }
   },
   "source": [
    "### Excercise 3 - Sparsity (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso can also be used for **feature selection** since L1 is [more likely to produce zero coefficients](https://explained.ai/regularization/).\n",
    "\n",
    "Is it indeed happening? \n",
    "\n",
    "Please do a discovery on your own and find that out empirically (both for **L1** and **L2**). Let's use `degree=15` and `alpha` from `ALPHAS`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-12-10T13:40:56.063816Z",
     "start_time": "2021-12-10T13:40:56.057832Z"
    }
   },
   "outputs": [],
   "source": [
    "# your findings/conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excercise 4 - Scaling (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a general rule, it is recommended to scale input features before fitting a regularized model so that the features/inputs take values in similar ranges. One common way of doing so is to standardize the inputs and that is exactly what our pipeline  second step (`StandardScaler`) is responsible for. \n",
    "\n",
    "Is scaling important? What are the underlying reasons?\n",
    "\n",
    "Please do a discovery on your own and find that out empirically (both for **L1** and **L2**) on the dataset below. Check coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_function_hw(x):\n",
    "    return 2 * x\n",
    "\n",
    "def generate_samples_hw():\n",
    "    np.random.seed(SEED)\n",
    "    x = np.random.uniform(*RANGE, size=N_SAMPLES)\n",
    "    \n",
    "    np.random.seed(SEED+1)\n",
    "    x_noise = np.random.uniform(*[x * 100 for x in RANGE], size=N_SAMPLES)\n",
    "    x_noise2 = np.random.normal(100, 50, size=N_SAMPLES)\n",
    "    \n",
    "    y = target_function_hw(x) + np.random.normal(scale=4, size=N_SAMPLES)\n",
    "    \n",
    "    return np.concatenate([x.reshape(-1, 1) / 100, \n",
    "                           x_noise.reshape(-1, 1),\n",
    "                           x_noise2.reshape(-1, 1)], axis=1), y\n",
    "\n",
    "X_hw, y_hw = generate_samples_hw()\n",
    "\n",
    "for i in range(X_hw.shape[1]):\n",
    "    print(f'Min of feature {i}: {min(X_hw[:, i]):.2f}, max: {max(X_hw[:, i]):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## your observations/conclusions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "288.188px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
