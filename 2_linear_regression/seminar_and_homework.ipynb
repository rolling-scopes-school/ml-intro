{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial data analysis and Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment is dedicated to Linear regression. By focusing on prediction different features of football players you understand the mathematics behind it and see the usefulness of main data analysis libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Materials**\n",
    "- [Documentation](http://docs.scipy.org/doc/) libraries Numpy and SciPy\n",
    "- [Documentation](http://matplotlib.org/) library Matplotlib \n",
    "- [Documentation](http://pandas.pydata.org/pandas-docs/stable/tutorials.html) library Pandas\n",
    "- [Pandas Cheat Sheet](http://www.analyticsvidhya.com/blog/2015/07/11-steps-perform-data-analysis-pandas-python/)\n",
    "- [Documentation](http://stanford.edu/~mwaskom/software/seaborn/) library Seaborn \n",
    "\n",
    "**Resources**\n",
    "- In this notebook we will use *FIFA 19 complete player dataset* which is taken from [here](https://www.kaggle.com/karangadiya/fifa19)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Initial data analysis with Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data. Table *data.csv* should be in the same directory as this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data.csv\", index_col='ID')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing you need to do with a dataframe after loading is to look at first few records. This way you can make sure that you have parsed it correctly. Moreover, you can get acquainted with the data, look at the features and their type (categorical, numerical, text ...).\n",
    "\n",
    "They you may check whether the data has missing values inside. Depending on the problem type and percentage of missing values you can either fill them with some value or drop columns/rows having null values.\n",
    "\n",
    "After that you may want to look closer at some features. You can draw a histogram for defining a feature distribution (normal, power or some other). Also with the help of histogram you can find values which are really differ from the rest, we call them **outliers**. Histograms can be plotted by *hist* method of Pandas DataFrame.\n",
    "\n",
    "**Example 1** Let's look at first 5 rows of data using method *head* for DataFrame data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately the number of columns exceeds the maximum visible default value in Pandas. Use the magic line above to remove this restriction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better now.\n",
    "\n",
    "**Example 2** Print total player number and top-10 columns containing the most number of null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total number of players in dataset {data.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "top = 10\n",
    "print(tabulate(\n",
    "    sorted(list(zip(data.columns, data.isnull().sum(), data.isnull().sum() / data.shape[0] * 100)), key=lambda x: -x[2])[:top], \n",
    "    headers=['col_name', 'null_cnt', 'null_perc']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 3**. Let's built a histogram of weight distribution in kgs from footbal players data. Follow steps:\n",
    "- Extract weight value from string (column Weight).\n",
    "- Convert *Weight* column to float type.\n",
    "- Get rid of null values in weight column, use median column value instead of them.\n",
    "- Convert pounds to kilograms\n",
    "- Finally use method *hist* for DataFrame *data* with arguments *column=Weight* (we look at this feature distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Weight column type is '{data['Weight'].dtype}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data['Weight_float'] = data['Weight'].str.extract(r'([0-9]+)lbs').astype(float)\n",
    "data['Weight_float'].fillna(data['Weight_float'].median())\n",
    "POUND_TO_KILO = 0.454\n",
    "data['Weight_kg'] = data.apply(lambda row: row['Weight_float'] * POUND_TO_KILO, axis=1)\n",
    "data.hist(column='Weight_kg', bins=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1 (1 point)**. Built a histogram of the height distribution in *meters* from footbal player data. Remember that height is in format *feet* '*inches*.  Instead of filling null values with some constant just drop them. Use *.dropna* for specified column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Effective way to visualize the relationship between two features is to draw a simple _scatter plot_. The position of each dot on the horizontal and vertical axis indicates values for an individual data point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 4.** Visualize the dependence of _Strength_  on _Weight_kg_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.plot.scatter(x='Weight_kg', y='Strength')\n",
    "plt.title('Dependence of strength on weight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One more effective way of initial data analysis is to plot pairwise feature dependencies. That simply combines already considered Scatter plot and a histogram. We create $m \\times m$ plots (_m_ is number of features) where pictures on diagonal represent **histograms** and outside the diagonal **scatter_matrix**. That can be done with the help of _scatter_matrix_ Pandas DataFrame method or _pairplot_ in Seaborn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 5.**\n",
    "Illustrate pairwise dependencies between _ShortPassing_, _Dribbling_, _BallControl_ and _Strength_ features of footbal players."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(data[['ShortPassing', 'Dribbling', 'BallControl', 'Strength']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Histograms and scatter plots are good for continuous (numerical) features. Distribution of data by categorical features (that have a fixed number of possible values) can be represented with **bar charts**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 6.** Show distribution of players by age groups (under 20 yo. _young_, between 20-30 _mature_, over 30 yo. _masters_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['age_group'] = data.apply(lambda x: 'young' if x['Age'] < 20 else 'mature' if x['Age'] <= 30 else 'masters', axis=1)\n",
    "distr = data.groupby('age_group').count().max(axis=1)[['young', 'mature', 'masters']]\n",
    "\n",
    "plt.bar(distr.index, distr.values)\n",
    "plt.ylabel('Number of players')\n",
    "plt.title('Distribution of players across age groups')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Really often it is necessary to explore the distribution of some numerical feature based on the value of categorical one. Here comes the _boxplot_ of Seaborn library, which can show statistics of numerical features (mean, quantiles) by different value of categorical feature. Boxplot can also help to detect **outliers** - values that significantly differ from the rest. More detailed explanation [here](https://towardsdatascience.com/understanding-boxplots-5e2df7bcbd51)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example 7.** Show _SprintSpeed_ statistics across different age groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Hint_: in order to prevent printing the service information and make our pictures more attractive we can write `;` in the end of last line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.boxplot(x='age_group', y='SprintSpeed', data=data);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Minimizing Mean Squared Error. Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to predict target numerical variable $y$ for _n_ samples with the help of $x_1, x_2, ..., x_m$ _m_ features under the assumption of _liner dependence_ existence between features and target, i.e.\n",
    "$$\\hat{y} = w_0 + w_1 * x_1 + w_2 * x_2 + ... + w_m * x_m$$\n",
    "so that Mean Squared Error between $y$ and $\\hat{y}$ was the lowest possible\n",
    "$$MSE = \\frac{1}{n}\\sum_{i=1}^n {(y_i - \\hat{y})}^2 -> min_{w_0, w_1, w_2, ...w_m}$$\n",
    "where $w_0$ is \"free\" weight component called **intercept** and $(w_1, w_2, ... w_n)$ is a **vector of coefficients**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Part 2.1 Linear Regression with one variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to understand the basic principles, let's try to predict _BallControl_ score based on the _Dribbling_ score for every player. Simple Linear Regression with one feature.\n",
    "$$BallControl = w_0 + w_1 * Dribbling$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to do real data science, aren't we? So let us split the available data into train and test samples. We let our model see only the train data, then we can measure it's quality on test sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data.fillna({'BallControl': data['BallControl'].mean(), 'Dribbling': data['Dribbling'].mean()}, inplace=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(data['Dribbling'].values, data['BallControl'].values, train_size=0.8)\n",
    "X_train = X_train.reshape(-1, 1)\n",
    "X_test = X_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate the approach, let's use Ridge model from sklearn with _regularization_ param alpha=0. What does it mean and what it if for we will find out later on in this course. But for now I require avoiding regularization by setting regularization param to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "lr = Ridge(alpha=0)\n",
    "lr.fit(X=X_train, y=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'w_0 = {lr.intercept_}, w_1 = {lr.coef_[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = lr.predict(X_train)\n",
    "y_pred_test = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['predicted_BallControl'] = lr.predict(data['Dribbling'].values.reshape(-1, 1))\n",
    "data[['Name', 'Dribbling', 'BallControl', 'predicted_BallControl']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now we have predictions for train and test samples. How about measure the quality of the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2 (0.5 point).** Write your own function for MSE calculation using the formula above. Calculate train and test MSE, compare to built-in method (_sklearn.metrics.mean_squared_error_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(y_true, y_pred):\n",
    "    error = # your code here\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "assert round(mean_squared_error(y_train, y_pred_train), 9) == round(mse(y_train, y_pred_train), 9)\n",
    "assert round(mean_squared_error(y_test, y_pred_test), 9) == round(mse(y_test, y_pred_test), 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Train MSE {mse(y_train, y_pred_train)}, test MSE {mse(y_test, y_pred_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3 (1.5 points).** Visualize the dependence of **test** _BallControl_ predictions and real _BallControl_ score on _Dribbling_ score. Don't forget to add axis and plot names!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.2 Linear regression with many variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4 (5 points).** Implement your own Linear Regression class for any number of input features and settable boolean parameter *fit_intercept*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task you will work with _optimize_ module of [_scipy_](https://docs.scipy.org/doc/scipy/reference/) open-source library for mathematics, science, and engineering. You will need a function [_least_squares_](https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.least_squares.html) that finds a coefficients for linear regression by minimizing the sum of the squares of the residuals (which is equivalent to MSE minimizing). More information about least squares approach [here](https://en.wikipedia.org/wiki/Least_squares). <br><br>\n",
    "Even though this function has many parameters, you need only a few of them to complete the task (the rest will be filled in with default values automatically).\n",
    "- **fun** computes a vector of residuals given weights, features and target, we provide you a function template _compute_residuals_\n",
    "- **x0** this is an initial weights vector. You can either pass a vector of zeros[n_features] or fill in randomly.\n",
    "- **args** are fixed arguments to _fun_ function (which we are not going to optimize). In that particular case you will need to pass X and y.\n",
    "\n",
    "\n",
    "You can access optimized weights by accessing the field **.x** of object which returns by this function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!! IMPORTANT <br>\n",
    "Please complete this assignment **without any cycles**. You may use the standard operations of matrix \\ vector multiplication ans different statistic calculation with NumPy. Otherwise, your solution may not go through asserts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_residuals(w, X, y):\n",
    "    \"\"\"\n",
    "    Compute residuals when predicting y_hat as matrix product of X and transposed w\n",
    "    :param w: linear regression weights, numpy.ndarrya: float64[num_features]\n",
    "    :param X: training features, numpy.ndarray: float64[num_samples, num_features]\n",
    "    :param y: training target, numpy.ndarray: float64[num_samples]\n",
    "    :returns: vector of residuals (y_i_hat - y_i) for each sample_i in X\n",
    "    \n",
    "    \"\"\"\n",
    "    residuals = # Your code here\n",
    "    return residuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "from scipy.optimize import least_squares\n",
    "\n",
    "class LinearRegression(BaseEstimator):\n",
    "    def __init__(self, fit_intercept=True):\n",
    "        self.fit_intercept = fit_intercept\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        fit model weights given input features and target\n",
    "        :param X: training features, numpy.ndarray: numeric[num_samples, num_features]\n",
    "        :param y: training target, numpy.ndarray: numeric[num_samples]\n",
    "        :returns: linear predictor with fitted weights so that train MSE is the lowest possible\n",
    "        :note: weights: numpy.ndarray: float64[num_features] stored as class field\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # Check that X and y have correct shape\n",
    "        X, y = check_X_y(X, y)\n",
    "        \n",
    "        # Save train data information. Necessary for following the uniform API\n",
    "        self.X_ = X\n",
    "        self.y_ = y\n",
    "        self.n_features_in_ = X.shape[1]\n",
    "        \n",
    "        # Copy arrays and cast them to uniform type\n",
    "        X_train = X.astype('float64')\n",
    "        y_train = y.astype('float64')\n",
    "        \n",
    "        # Add dummy column of ones to X_train if we want to train an intercept - last component of future weight vector\n",
    "        if self.fit_intercept:\n",
    "            X_train = np.column_stack((X_train, np.ones(X_train.shape[0])))\n",
    "        \n",
    "        # Your code here.\n",
    "        # Just follow the suggested steps: create initial weights vector,\n",
    "        # apply least_squares optimizer passing the parameters described above\n",
    "        # and finally extract optimized weights.\n",
    "        # Remember: you need to distinguish coefficients from intercept when fit_intercept=True\n",
    "        self.coef_ = \n",
    "        self.intercept_ = \n",
    "        \n",
    "        # Return the classifier\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        # Check is fit had been called\n",
    "        check_is_fitted(self)\n",
    "        \n",
    "        # Input validation\n",
    "        X = check_array(X)\n",
    "        \n",
    "        return X.dot(self.coef_) + self.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Testing area\n",
    "from sklearn.utils.estimator_checks import check_estimator\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "lr = LinearRegression()\n",
    "ridge = Ridge(alpha=0)\n",
    "lr_no_intercept = LinearRegression(fit_intercept=False)\n",
    "ridge_no_intercept = Ridge(alpha=0, fit_intercept=False)\n",
    "\n",
    "#Check compatibility with Sklearn framework and apply some spesific internal tests\n",
    "check_estimator(lr)\n",
    "check_estimator(lr_no_intercept)\n",
    "\n",
    "#Compare model accuracy with Ridge(0) from Sklearn\n",
    "data.fillna({'BallControl': data['BallControl'].mean()\n",
    "             , 'Dribbling': data['Dribbling'].mean()\n",
    "             , 'Strength': data['Strength'].mean()}, inplace=True)\n",
    "X_sample, y_sample = data[['Dribbling', 'Strength']], data['BallControl']\n",
    "lr.fit(X_sample, y_sample)\n",
    "ridge.fit(X_sample, y_sample)\n",
    "assert np.allclose(lr.predict(X_sample), ridge.predict(X_sample), rtol=1e-03), \"Your model with intercept not accurate enough!\"\n",
    "lr_no_intercept.fit(X_sample, y_sample)\n",
    "ridge_no_intercept.fit(X_sample, y_sample)\n",
    "assert np.allclose(lr_no_intercept.predict(X_sample), ridge_no_intercept.predict(X_sample), rtol=1e-03), \"Your model without intercept not accurate enough!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add more features in order to predict Dribbling score more accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['BallControl', 'ShortPassing', 'Strength', 'Weight_float', 'Weight_kg']\n",
    "target = 'Dribbling'\n",
    "for feat in features:\n",
    "    data.fillna({feat: data[feat].mean()}, inplace=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[features].values, data[target].values, train_size=0.8, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = Ridge(0)\n",
    "lr.fit(X=X_train, y=y_train)\n",
    "\n",
    "y_pred_train = lr.predict(X_train)\n",
    "y_pred_test = lr.predict(X_test)\n",
    "\n",
    "print(f'Train MSE {mean_squared_error(y_train, y_pred_train)}, test MSE {mean_squared_error(y_test, y_pred_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'w_0 = {lr.intercept_}, w_1, w_2, w_3, w_4, w_5 = {lr.coef_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is not ok, two last weight components are too large, and they vary depending on the run! Although the result seems better our model would behave unexpectadly to the patterns in data it has never seen! Large weights and weights instability are the sign of [**overfitting**](https://en.wikipedia.org/wiki/Overfitting).  <br><br>\n",
    "According to the definition it is \"_the production of an analysis that corresponds too closely or exactly to a particular set of data, and may therefore fail to fit additional data or predict future observations reliably_\". But what does it actually mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assume that we have a player whose weight in kg was calculated with some tiny error, let's say +=1g."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player = data[features + [target]].iloc[0:2]\n",
    "player['Predicted_dribbling'] = lr.predict(player[features].values)\n",
    "player.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions are pretty good if the data is _pure_. Let's add some noise to _Weight_kg_ feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player['Weight_kg'] = player['Weight_kg'] + [-0.001, 0.001]\n",
    "player['Predicted_dribbling_with_error'] = lr.predict(player[features].values)\n",
    "player.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicted dribbling value has changed significantly! Look at how this tiny **1g** error leads to extremly big or small dribbling!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason behind it is strange unstable behaviour is **collinearity** between Weight and Weight_kg features, what means that Weight_kg can be linearly predicted from Weight. As a matter of fact they represent the same essense but in different scales. <br><br>\n",
    "**Multicollinearity** describes a more general case, when one feature can be predicted by linear combination of some other features.<br><br>\n",
    "Collinearity is really close related to **correlation** - degree to which a pair of variables are linearly related. Collinearity origins from Linear Algebra and Geometry whereas Correlation is a term from Statistics. Anyway all of this three terms refer to **linearly dependent features**, which is really bad for Linear Models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But why it is so bad? The main reason is that Linear Regression tries to capture the contribution of each feature to target _independently_, which obviously is not possible in terms of feature multicolliearity.<br><br>\n",
    "There are a whole bunch of really interesting thoughts that can help to capture the intuition behind it [here](https://stats.stackexchange.com/questions/1149/is-there-an-intuitive-explanation-why-multicollinearity-is-a-problem-in-linear-r). I'd citate one of the examples provided.<br><br>\n",
    "_Assume that two people collaborated and accomplished scientific discovery. It is easy to tell their unique contributions (who did what) when two are totally different persons (one is theory guy and the other is good at experiment), while it is difficult to distinguish their unique influences (coefficients in regression) when they are twins acting similarly._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few approaches how to prevent overfitting and overcome multicollinearity.\n",
    "- Drop features\n",
    "- Combine features\n",
    "- Regularization\n",
    "\n",
    "\n",
    "Regularization is something we are going to speak about in the next modules. Combining features is problem-specific and could easily trigger a _holy_war_ due to ambiguity of approaches. Let's focus on simpliest - drop one of the features from the correlated pair.<br>\n",
    "At first we need to define those pairs of features, **correlation matrix** comes to rescue! Each cell in the table shows the correlation between two variables. We use dataframe in-built method _corr_ in combination with seaborn _heatmap_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seaborn import heatmap\n",
    "\n",
    "heatmap(data[features].corr(method='pearson'), center=0, square=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['BallControl', 'ShortPassing', 'Strength', 'Weight_kg']\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[features].values, data[target].values, train_size=0.8, random_state=2)\n",
    "\n",
    "lr = Ridge(alpha=0)\n",
    "lr.fit(X=X_train, y=y_train)\n",
    "\n",
    "player['Predicted_dribbling_with_error'] = lr.predict(player[features].values)\n",
    "player.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2.3 Putting all together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 5 (up to 5 points).** Build a Linear Regression model for _Value_ prediction for every football player and validate it. You **have to** use either your custom Linear Regression class or `sklearn.linear_model.Ridge` with regularization param alpha=0. Steps you need to follow:\n",
    "- Extract float number from _Value_ field in DataFrame (**0.5 points**)\n",
    "- Сhoose more features that you expect to influence on player _Value_ (at least 10)\n",
    "- Plot feature correlation matrix. (**0.5 points**)\n",
    "- Drop features that are highly correlated with each other (_abs_(corr) > 0.9) one by one until no correlated pairs left. _Hint_: you may reuse code from Task_9 in HW_1 for automatic correlated pairs selection. (**1.5 points**)\n",
    "- Split data into train/test with some proportion (**0.5 points**)\n",
    "- Train a model on train dataset, make predictions both for train and test. (**0.5 points**)\n",
    "- Measure the model quality in terms of MSE in train and test samples,  (**0.5 points**)\n",
    "- Write a short report about the work done. Why did you take these particular features? Can you find a logical explanation for high correlation of some of your features? Are you satisfied with the quality of predictions? etc. (**1 point**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Penalties**\n",
    "- **-1 point** if used a different model besides custom Linear Regression or `sklearn.linear_model.Ridge` with regularization param alpha=0\n",
    "- **-0.5 points** if number of selected features BEFORE removal of linearly dependent ones is less than 10.\n",
    "- **-0.5 points** if did not remove linearly dependent features before training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code and a bunch of cool ideas here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
